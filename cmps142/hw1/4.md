Hongze Xu
hoxu@ucsc.edu

To solve

$$
IG(x,y) =H[x] - H[x|y]
$$

We have known that 
$$
H[x|y] = \sum_{y \in Y} p(y) H(X|Y=y) 
$$ from definition of conditional entropy.

We can derive that

$$
\sum_{y \in Y} p(y) H(X|Y=y) = -\sum_{y \in Y} p(y) \sum_{x \in X} p(x|y) log_2 {p(x|y)}
$$


$$
= -\sum_{x \in X} \sum_{y \in Y} p(y) p(x|y) log_2 {p(x|y)}
$$


from definition of conditional probability $p(x|y) = \frac {p(x,y)} {p(y)}$, which derive $p(x|y) {p(y)} = {p(x,y)}$. Therefore

$$
H[x|y] = -\sum_{x \in X} \sum_{y \in Y} p(x,y) log_2 {p(x|y)}
$$

We known that
$$
H[x] = -\sum_{x \in X} p(x)log_2 {p(x)}
$$

from definition of entropy. Then we can expand it by marginalization:

$$
-\sum_{x \in X} p(x)log_2 {p(x)} = -\sum_{x \in X} (\sum_{y \in Y} p(x, y) ) log_2 p(x) = -\sum_{x \in X} \sum_{y \in Y} p(x, y)  log_2 p(x)
$$

Therefore,
$$
H[x] = -\sum_{x \in X} \sum_{y \in Y} p(x, y)  log_2 p(x)
$$

Combining two of them

$$
IG(x,y) = -\sum_{x \in X} \sum_{y \in Y} p(x, y)  log_2 p(x) +\sum_{x \in X} \sum_{y \in Y} p(x,y) log_2 {p(x|y)}
$$


$$
= -\sum_{x \in X} \sum_{y \in Y} p(x,y) (log_2 p(x)-   log_2 {p(x|y)})
$$

$$
= -\sum_{x \in X} \sum_{y \in Y} p(x,y) log_2 \frac {p(x)} {{p(x|y)} }
$$

$$
= -\sum_{x \in X} \sum_{y \in Y} p(x,y) log_2 \frac {p(x)p(y)} {{p(x,y)} }
$$

which is the same expression derived by $KL(p||q)$